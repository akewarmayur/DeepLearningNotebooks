{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM8NuuUR5O/LY+HFmU9KjYS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**This notebook contans the gradio app for displaying yolo v8 model results**"],"metadata":{"id":"kwppEflsiCxZ"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sLk10Zj3h-T3","executionInfo":{"status":"ok","timestamp":1689843799822,"user_tz":-330,"elapsed":32115,"user":{"displayName":"mike yolo","userId":"10130328056325800129"}},"outputId":"8915f8e2-669f-4f2d-85f9-178ba33e4ea3"},"outputs":[{"output_type":"stream","name":"stderr","text":["Ultralytics YOLOv8.0.138 ðŸš€ Python-3.10.6 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","Setup complete âœ… (2 CPUs, 12.7 GB RAM, 24.4/78.2 GB disk)\n"]}],"source":["!pip install ultralytics\n","!pip install -q gradio\n","from IPython import display\n","display.clear_output()\n","\n","import ultralytics\n","ultralytics.checks()"]},{"cell_type":"code","source":["!wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KOKkRcPDiRBY","executionInfo":{"status":"ok","timestamp":1689843805695,"user_tz":-330,"elapsed":899,"user":{"displayName":"mike yolo","userId":"10130328056325800129"}},"outputId":"c89d4b17-e526-4b36-d952-977217bcb759"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-07-20 09:03:24--  https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/truck.jpg\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 271475 (265K) [image/jpeg]\n","Saving to: â€˜images/truck.jpgâ€™\n","\n","truck.jpg           100%[===================>] 265.11K  --.-KB/s    in 0.004s  \n","\n","2023-07-20 09:03:24 (67.4 MB/s) - â€˜images/truck.jpgâ€™ saved [271475/271475]\n","\n"]}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from ultralytics import YOLO\n","import os\n","from moviepy.editor import *\n","from google.colab.patches import cv2_imshow\n","import moviepy.editor as mp\n","import gradio as gr"],"metadata":{"id":"T05kcm22iZlg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_video(input_video):\n","\n","  # Step 2: Extract frames\n","  video = cv2.VideoCapture(input_video)\n","  fps = video.get(cv2.CAP_PROP_FPS)\n","  frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n","  frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","  output_video = \"/content/gdrive/My Drive/demoM/test_images/output_video.mp4\"\n","  output_writer = cv2.VideoWriter(output_video, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (frame_width, frame_height))\n","\n","  # Step 3: Pass each frame to a model to detect objects\n","  model = YOLO(\"/content/gdrive/My Drive/demoM/trainedModel/bestV1.pt\")\n","\n","  # Step 4: Extract audio from the input video\n","  video_clip = mp.VideoFileClip(input_video)\n","  audio_clip = video_clip.audio\n","  audio_clip.write_audiofile(\"temp_audio.wav\")\n","\n","  while True:\n","      # Read the next frame\n","      try:\n","        ret, frame = video.read()\n","        if not ret:\n","            break\n","        # Step 5: Get the mask of the object and blur the mask\n","        results = model.predict(source=frame, conf=0.5)\n","        objects_mask = results[0].masks.xy\n","        mask_ = np.zeros_like(frame)\n","        for mask in objects_mask:\n","          vertices = mask.reshape(mask.shape[0], mask.shape[1] // 2, 2)\n","          cv2.fillPoly(mask_, np.int32([vertices]), (255, 255, 255))\n","          #blurred_region1 = cv2.blur(frame, (20, 20), 10)\n","          blurred_region2 = cv2.GaussianBlur(frame, (53, 53), 30)\n","          output_frame = np.where(mask_ != 0, blurred_region2, frame)\n","        output_writer.write(output_frame)\n","      except Exception as e:\n","        output_writer.write(frame)\n","\n","  # Release the video capture and writer, and close windows\n","  video.release()\n","  output_writer.release()\n","\n","  # Step 8: Merge the output video with the audio using moviepy\n","  final_output = mp.VideoFileClip(output_video)\n","  final_output = final_output.set_audio(mp.AudioFileClip(\"temp_audio.wav\"))\n","  final_output.write_videofile(\"/content/gdrive/My Drive/demoM/test_images/final_output_video.mp4\", codec=\"libx264\", audio_codec=\"aac\", remove_temp=True)\n","  return \"/content/gdrive/My Drive/demoM/test_images/final_output_video.mp4\"\n","\n","\n","# Define the input and output components\n","input_video = gr.inputs.Video(type=\"mp4\", label=\"Input Video\")\n","output_video = gr.outputs.File(label=\"Processed Video\")\n","\n","# Create the Gradio interface\n","interface = gr.Interface(fn=process_video, inputs=input_video, outputs=output_video)\n","\n","# Launch the interface\n","interface.launch(share=True)"],"metadata":{"id":"1hZDZpkiijhU"},"execution_count":null,"outputs":[]}]}